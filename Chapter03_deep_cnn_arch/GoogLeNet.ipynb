{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring GoogLeNet and Inception v3\n",
    "\n",
    "As we have discovered the progression of CNN models from LeNet to VGG so far, we have observed the sequential stacking of more convolutional and fully connected layers. This resulted in deep networks with a lot of parameters to train. GoogLeNet emerged as a radically different type of CNN architecture that is composed of a module of parallel convolutional layers called the inception module. Because of this, GoogLeNet is also called Inception v1 (v1 marked the first version as more versions came along later). Some of the drastically new elements introduced in GoogLeNet were the following:\n",
    "\n",
    "- The inception module – a module of several parallel convolutional layers\n",
    "- Using 1x1 convolutions to reduce the number of model parameters\n",
    "- Global average pooling instead of a fully connected layer – reduces overfitting\n",
    "- Using auxiliary classifiers for training – for regularization and gradient stability\n",
    "\n",
    "GoogLeNet has 22 layers, which is more than the number of layers of any VGG model variant. Yet, due to some of the optimization tricks used, the number of parameters in GoogLeNet is 5 million, which is far less than the 138 million parameters of VGG. Let's expand on some of the key features of this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inception modules\n",
    "\n",
    "Perhaps the single most important contribution of this model was the development of a convolutional module with several convolutional layers running in parallel, which are finally concatenated to produce a single output vector. These parallel convolutional layers operate with different kernel sizes ranging from 1x1 to 3x3 to 5x5. The idea is to extract all levels of visual information from the image. Besides these convolutions, a 3x3 max-pooling layer adds another level of feature extraction. Figure 3.24 shows the inception block diagram along with the overall GoogLeNet architecture:\n",
    "![img](B12158_03_24.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InceptionModule(nn.Module):\n",
    "    def __init__(self, input_planes, n_channels1x1, n_channels3x3red, n_channels3x3, n_channels5x5red, n_channels5x5, pooling_planes):\n",
    "        super(InceptionModule, self).__init__()\n",
    "        # 1x1 convolution branch\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(input_planes, n_channels1x1, kernel_size=1),\n",
    "            nn.BatchNorm2d(n_channels1x1),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    " \n",
    "        # 1x1 convolution -> 3x3 convolution branch\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Conv2d(input_planes, n_channels3x3red, kernel_size=1),\n",
    "            nn.BatchNorm2d(n_channels3x3red),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(n_channels3x3red, n_channels3x3, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(n_channels3x3),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    " \n",
    "        # 1x1 conv -> 5x5 conv branch\n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.Conv2d(input_planes, n_channels5x5red, kernel_size=1),\n",
    "            nn.BatchNorm2d(n_channels5x5red),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(n_channels5x5red, n_channels5x5, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(n_channels5x5),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(n_channels5x5, n_channels5x5, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(n_channels5x5),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    " \n",
    "        # 3x3 pool -> 1x1 conv branch\n",
    "        self.block4 = nn.Sequential(\n",
    "            nn.MaxPool2d(3, stride=1, padding=1),\n",
    "            nn.Conv2d(input_planes, pooling_planes, kernel_size=1),\n",
    "            nn.BatchNorm2d(pooling_planes),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    " \n",
    "    def forward(self, ip):\n",
    "        op1 = self.block1(ip)\n",
    "        op2 = self.block2(ip)\n",
    "        op3 = self.block3(ip)\n",
    "        op4 = self.block4(ip)\n",
    "        return torch.cat([op1,op2,op3,op4], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoogLeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GoogLeNet, self).__init__()\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(3, 192, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(192),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    " \n",
    "        self.im1 = InceptionModule(192,  64,  96, 128, 16, 32, 32)\n",
    "        self.im2 = InceptionModule(256, 128, 128, 192, 32, 96, 64)\n",
    " \n",
    "        self.max_pool = nn.MaxPool2d(3, stride=2, padding=1)\n",
    " \n",
    "        self.im3 = InceptionModule(480, 192,  96, 208, 16,  48,  64)\n",
    "        self.im4 = InceptionModule(512, 160, 112, 224, 24,  64,  64)\n",
    "        self.im5 = InceptionModule(512, 128, 128, 256, 24,  64,  64)\n",
    "        self.im6 = InceptionModule(512, 112, 144, 288, 32,  64,  64)\n",
    "        self.im7 = InceptionModule(528, 256, 160, 320, 32, 128, 128)\n",
    " \n",
    "        self.im8 = InceptionModule(832, 256, 160, 320, 32, 128, 128)\n",
    "        self.im9 = InceptionModule(832, 384, 192, 384, 48, 128, 128)\n",
    " \n",
    "        self.average_pool = nn.AvgPool2d(7, stride=1)\n",
    "        self.fc = nn.Linear(4096, 1000)\n",
    " \n",
    "    def forward(self, ip):\n",
    "        op = self.stem(ip)\n",
    "        out = self.im1(op)\n",
    "        out = self.im2(op)\n",
    "        op = self.maxpool(op)\n",
    "        op = self.a4(op)\n",
    "        op = self.b4(op)\n",
    "        op = self.c4(op)\n",
    "        op = self.d4(op)\n",
    "        op = self.e4(op)\n",
    "        op = self.max_pool(op)\n",
    "        op = self.a5(op)\n",
    "        op = self.b5(op)\n",
    "        op = self.avgerage_pool(op)\n",
    "        op = op.view(op.size(0), -1)\n",
    "        op = self.fc(op)\n",
    "        return op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1x1 convolutions\n",
    "In addition to the parallel convolutional layers in an inception module, each parallel layer has a preceding 1x1 convolutional layer. The reason behind using these 1x1 convolutional layers is dimensionality reduction. 1x1 convolutions do not change the width and height of the image representation but can alter the depth of an image representation. This trick is used to reduce the depth of the input visual features before performing the 1x1, 3x3, and 5x5 convolutions parallelly. Reducing the number of parameters not only helps build a lighter model but also combats overfitting.\n",
    "\n",
    "## Global average pooling\n",
    "If we look at the overall GoogLeNet architecture in Figure 3.24, the penultimate output layer of the model is preceded by a 7x7 average pooling layer. This layer again helps in reducing the number of parameters of the model, thereby reducing overfitting. Without this layer, the model would have millions of additional parameters due to the dense connections of a fully connected layer.\n",
    "\n",
    "## Auxiliary classifiers\n",
    "Figure 3.24 also shows two extra or auxiliary output branches in the model. These auxiliary classifiers are supposed to tackle the vanishing gradient problem by adding to the gradients' magnitude during backpropagation, especially for the layers towards the input end. Because these models have a large number of layers, vanishing gradients can become a bottleneck. Hence, using auxiliary classifiers has proven useful for this 22-layer deep model. Additionally, the auxiliary branches also help in regularization. Please note that these auxiliary branches are switched off/discarded while making predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inception V3\n",
    "\n",
    "This successor of Inception v1 has a total of 24 million parameters as compared to 5 million in v1. Besides the addition of several more layers, this model introduced different kinds of inception modules, which are stacked sequentially. Figure 3.25 shows the different inception modules and the full model architecture:\n",
    "![img](B12158_03_25.jpg)\n",
    "\n",
    "It can be seen from the architecture that this model is an architectural extension of the Inception v1 model. Once again, besides building the model manually, we can use the pre-trained model from PyTorch's repository as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "model = models.inception_v3(pretrained=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
