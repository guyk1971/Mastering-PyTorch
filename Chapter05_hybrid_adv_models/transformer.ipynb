{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a transformer model for language modeling\n",
    "In this section, we will explore what transformers are and build one using PyTorch for the task of language modeling. We will also learn how to use some of its successors, such as BERT and GPT, via PyTorch's pretrained model repository. Before we start building a transformer model, let's quickly recap what language modeling is.\n",
    "\n",
    "Reviewing language modeling\n",
    "Language modeling is the task of figuring out the probability of the occurrence of a word or a sequence of words that should follow a given sequence of words. For example, if we are given French is a beautiful _____ as our sequence of words, what is the probability that the next word will be language or word, and so on? These probabilities are computed by modeling the language using various probabilistic and statistical techniques. The idea is to observe a text corpus and learn the grammar by learning which words occur together and which words never occur together. This way, a language model establishes probabilistic rules around the occurrence of different words or sequences, given various different sequences.\n",
    "\n",
    "Recurrent models have been a popular way of learning a language model. However, as with many sequence-related tasks, transformers have outperformed recurrent networks on this task as well. We will implement a transformer-based language model for the English language by training it on the Wikipedia text corpus.\n",
    "\n",
    "Now, let's start training a transformer for language modeling. \n",
    "\n",
    "We will delve deeper into the various components of the transformer architecture in-between the exercise.\n",
    "\n",
    "For this exercise, we will need to import a few dependencies. One of the important import statements is listed here:\n",
    "Besides importing the regular torch dependencies, we must import some modules specific to the transformer model; these are provided directly under the torch library. We'll also import torchtext in order to download a text dataset directly from the available datasets under `torchtext.datasets`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "import torchtext\n",
    "from torchtext.data.utils import get_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding the transformer model architecture\n",
    "This is perhaps the most important step of this exercise. Here, we define the architecture of the transformer model.\n",
    "\n",
    "First, let's briefly discuss the model architecture and then look at the PyTorch code for defining the model. The following diagram shows the model architecture:\n",
    "![img](B12158_05_01.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, num_token, num_inputs, num_heads, num_hidden, num_layers, dropout=0.3):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.model_name = 'transformer'\n",
    "        self.mask_source = None\n",
    "        self.position_enc = PosEnc(num_inputs, dropout)\n",
    "        layers_enc = TransformerEncoderLayer(num_inputs, num_heads, num_hidden, dropout)\n",
    "        self.enc_transformer = TransformerEncoder(layers_enc, num_layers)\n",
    "        self.enc = nn.Embedding(num_token, num_inputs)\n",
    "        self.num_inputs = num_inputs\n",
    "        self.dec = nn.Linear(num_inputs, num_token)\n",
    "        self.init_params()\n",
    "\n",
    "    def _gen_sqr_nxt_mask(self, size):\n",
    "        msk = (torch.triu(torch.ones(size, size)) == 1).transpose(0, 1)\n",
    "        msk = msk.float().masked_fill(msk == 0, float('-inf'))\n",
    "        msk = msk.masked_fill(msk == 1, float(0.0))\n",
    "        return msk\n",
    "\n",
    "    def init_params(self):\n",
    "        initial_rng = 0.12\n",
    "        self.enc.weight.data.uniform_(-initial_rng, initial_rng)\n",
    "        self.dec.bias.data.zero_()\n",
    "        self.dec.weight.data.uniform_(-initial_rng, initial_rng)\n",
    "\n",
    "    def forward(self, source):\n",
    "        if self.mask_source is None or self.mask_source.size(0) != len(source):\n",
    "            dvc = source.device\n",
    "            msk = self._gen_sqr_nxt_mask(len(source)).to(dvc)\n",
    "            self.mask_source = msk\n",
    "\n",
    "        source = self.enc(source) * math.sqrt(self.num_inputs)\n",
    "        source = self.position_enc(source)\n",
    "        op = self.enc_transformer(source, self.mask_source)\n",
    "        op = self.dec(op)\n",
    "        return op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing to notice is that this is essentially an encoder-decoder based architecture, with the Encoder Unit on the left (in purple) and the Decoder Unit (in orange) on the right. The encoder and decoder units can be tiled multiple times for even deeper architectures. In our example, we have two cascaded encoder units and a single decoder unit. This encoder-decoder setup essentially means that the encoder takes a sequence as input and generates as many embeddings as there are words in the input sequence (that is, one embedding per word). These embeddings are then fed to the decoder, along with the predictions made thus far by the model.\n",
    "\n",
    "Let's walk through the various layers in this model:\n",
    "\n",
    "**Embedding Layer**: This layer is simply meant to perform the traditional task of converting each input word of the sequence into a vector of numbers; that is, an embedding. As always, here, we use the torch.nn.Embedding module to code this layer.\n",
    "\n",
    "**Positional Encoder**: Note that transformers do not have any recurrent layers in their architecture, yet they outperform recurrent networks on sequential tasks. How? Using a neat trick known as positional encoding, the model is provided the sense of sequentiality or sequential-order in the data. Basically, vectors that follow a particular sequential pattern are added to the input word embeddings.These vectors are generated in a way that enables the model to understand that the second word comes after the first word and so on. The vectors are generated using the sinusoidal and cosinusoidal functions to represent a systematic periodicity and distance between subsequent words, respectively. The implementation of this layer for our exercise is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PosEnc(nn.Module):\n",
    "    def __init__(self, d_m, dropout=0.2, size_limit=5000):\n",
    "        # d_m is same as the dimension of the embeddings\n",
    "        super(PosEnc, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        p_enc = torch.zeros(size_limit, d_m)\n",
    "        pos = torch.arange(0, size_limit, dtype=torch.float).unsqueeze(1)\n",
    "        divider = torch.exp(torch.arange(0, d_m, 2).float() * (-math.log(10000.0) / d_m))\n",
    "        # divider is the list of radians, multiplied by position indices of words, and fed to the sinusoidal and cosinusoidal function\n",
    "        p_enc[:, 0::2] = torch.sin(pos * divider)\n",
    "        p_enc[:, 1::2] = torch.cos(pos * divider)\n",
    "        p_enc = p_enc.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('p_enc', p_enc)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dropout(x + self.p_enc[:x.size(0), :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the sinusoidal and cosinusoidal functions are used alternatively to give the sequential pattern. There are many ways to implement positional encoding though. Without a positional encoding layer, the model will be clueless about the order of the words.\n",
    "\n",
    "**Multi-Head Attention**: Before we look at the multi-head attention layer, let's first understand what a self-attention layer is. We covered the concept of attention in Chapter 4, Deep Recurrent Model Architectures, with respect to recurrent networks. Here, as the name suggests, the attention mechanism is applied to self; that is, each word of the sequence. Each word embedding of the sequence goes through the self-attention layer and produces an individual output that is exactly the same length as the word embedding. The following diagram describes the process of this in detail:\n",
    "\n",
    "![img](B12158_05_02.jpg)\n",
    "\n",
    "As we can see, for each word, three vectors are generated through three learnable parameter matrices (Pq, Pk, and Pv). The three vectors are query, key, and value vectors. The query and key vectors are dot-multiplied to produce a number for each word. These numbers are normalized by dividing the square root of the key vector length for each word. The resultant numbers for all words are then Softmaxed at the same time to produce probabilities that are finally multiplied by the respective value vectors for each word. This results in one output vector for each word of the sequence, with the lengths of the output vector and the input word embedding being the same.\n",
    "\n",
    "A multi-head attention layer is an extension of the self-attention layer where multiple self-attention modules compute outputs for each word. These individual outputs are concatenated and matrix-multiplied with yet another parameter matrix (Pm) to generate the final output vector, whose length is equal to the input embedding vector's. The following diagram shows the multi-head attention layer, along with two self-attention units that we will be using in this exercise:\n",
    "\n",
    "![img](B12158_05_03.jpg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having multiple self-attention heads helps different heads focus on different aspects of the sequence word, similar to how different feature maps learn different patterns in a convolutional neural network. Due to this, the multi-head attention layer performs better than an individual self-attention layer and will be used in our exercise.\n",
    "\n",
    "Also, note that the masked multi-head attention layer in the decoder unit works in exactly the same way as a multi-head attention layer, except for the added masking; that is, given time step t of processing the sequence, all words from t+1 to n (length of the sequence) are masked/hidden.\n",
    "\n",
    "During training, the decoder is provided with two types of inputs. On one hand, it receives query and key vectors from the final encoder as inputs to its (unmasked) multi-head attention layer, where these query and key vectors are matrix transformations of the final encoder output. On the other hand, the decoder receives its own predictions from previous time steps as sequential input to its masked multi-head attention layer.\n",
    "\n",
    "**Addition and Layer Normalization**: We discussed the concept of a residual connection in Chapter 3, Deep CNN Architectures, while discussing ResNets. In Figure 5.1, we can see that there are residual connections across the addition and layer normalization layers. In each instance, a residual connection is established by directly adding the input word embedding vector to the output vector of the multi-head attention layer. This helps with easier gradient flow throughout the network and avoiding problems with exploding and vanishing gradients. Also, it helps with efficiently learning identity functions across layers.Furthermore, layer normalization is used as a normalization trick. Here, we normalize each feature independently so that all the features have a uniform mean and standard deviation. Please note that these additions and normalizations are applied individually to each word vector of the sequence at each stage of the network.\n",
    "\n",
    "**Feedforward Layer**: Within both the encoder and decoder units, the normalized residual output vectors for all the words of the sequence are passed through a common feedforward layer. Due to there being a common set of parameters across words, this layer helps with learning broader patterns across the sequence.\n",
    "\n",
    "**Linear and Softmax Layer**: So far, each layer is outputting a sequence of vectors, one per word. For our task of language modeling, we need a single final output. The linear layer transforms the sequence of vectors into a single vector whose size is equal to the length of our word vocabulary. The Softmax layer converts this output into a vector of probabilities summing to 1. These probabilities are the probabilities that the respective words (in the vocabulary) occur as the next words in the sequence.\n",
    "Now that we have elaborated on the various elements of a transformer model, let's look at the PyTorch code for instantiating the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, in the `__init__` method of the class, thanks to PyTorch's `TransformerEncoder` and `TransformerEncoderLayer` functions, we do not need to implement these ourselves. For our language modeling task, we just need a single output for the input sequence of words. Due to this, the decoder is just a linear layer that transforms the sequence of vectors from an encoder into a single output vector. A position encoder is also initialized using the definition that we discussed earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and processing the dataset\n",
    "In this section, we will discuss the steps related to loading a text dataset for our task and making it usable for the model training routine. Let's get started:\n",
    "\n",
    "For this exercise, we will be using texts from Wikipedia, all of which are available as the WikiText-2 dataset.\n",
    "Dataset Citation\n",
    "\n",
    "https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/.\n",
    "\n",
    "We'll use the functionality of torchtext to download the dataset (available under torchtext datasets), tokenize its vocabulary, and split the dataset into training, validation, and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = torchtext.data.Field(tokenize=get_tokenizer(\"basic_english\"), lower=True, eos_token='<eos>', init_token='<sos>')\n",
    "training_text, validation_text, testing_text = torchtext.datasets.WikiText2.splits(TEXT)\n",
    "TEXT.build_vocab(training_text)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def gen_batches(text_dataset, batch_size):\n",
    "    text_dataset = TEXT.numericalize([text_dataset.examples[0].text])\n",
    "    # divide text dataset into parts of size equal to batch_size\n",
    "    num_batches = text_dataset.size(0) // batch_size\n",
    "    # remove data points that lie outside batches (remainders)\n",
    "    text_dataset = text_dataset.narrow(0, 0, num_batches * batch_size)\n",
    "    # distribute dataset across batches evenly\n",
    "    text_dataset = text_dataset.view(batch_size, -1).t().contiguous()\n",
    "    return text_dataset.to(device)\n",
    "\n",
    "training_batch_size = 32\n",
    "evaluation_batch_size = 16\n",
    "\n",
    "training_data = gen_batches(training_text, training_batch_size)\n",
    "validation_data = gen_batches(validation_text, evaluation_batch_size)\n",
    "testing_data = gen_batches(testing_text, evaluation_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we must define the maximum sequence length and write a function that will generate input sequences and output targets for each batch, accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 64\n",
    "def return_batch(src, k):\n",
    "    sequence_length = min(max_seq_len, len(src) - 1 - k)\n",
    "    sequence_data = src[k:k+sequence_length]\n",
    "    sequence_label = src[k+1:k+1+sequence_length].view(-1)\n",
    "    return sequence_data, sequence_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the transformer model\n",
    "In this section, we will define the necessary hyperparameters for model training, define the model training and evaluation routines, and finally execute the training loop. Let's get started:\n",
    "\n",
    "In this step, we define all the model hyperparameters and instantiate our transformer model. The following code is self-explanatory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tokens = len(TEXT.vocab.stoi) # vocabulary size\n",
    "embedding_size = 256 # dimension of embedding layer\n",
    "num_hidden_params = 256 # transformer encoder's hidden (feed forward) layer dimension\n",
    "num_layers = 2 # num of transformer encoder layers within transformer encoder\n",
    "num_heads = 2 # num of heads in (multi head) attention models\n",
    "dropout = 0.25 # value (fraction) of dropout\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "lrate = 4.0 # learning rate\n",
    "transformer_model = Transformer(num_tokens, embedding_size, num_heads, num_hidden_params, num_layers, \n",
    "                                     dropout).to(device)\n",
    "optim_module = torch.optim.SGD(transformer_model.parameters(), lr=lrate)\n",
    "sched_module = torch.optim.lr_scheduler.StepLR(optim_module, 1.0, gamma=0.88)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    transformer_model.train()\n",
    "    loss_total = 0.\n",
    "    time_start = time.time()\n",
    "    num_tokens = len(TEXT.vocab.stoi)\n",
    "    for b, i in enumerate(range(0, training_data.size(0) - 1, max_seq_len)):\n",
    "        train_data_batch, train_label_batch = return_batch(training_data, i)\n",
    "        optim_module.zero_grad()\n",
    "        op = transformer_model(train_data_batch)\n",
    "        loss_curr = loss_func(op.view(-1, num_tokens), train_label_batch)\n",
    "        loss_curr.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(transformer_model.parameters(), 0.6)\n",
    "        optim_module.step()\n",
    "\n",
    "        loss_total += loss_curr.item()\n",
    "        interval = 100\n",
    "        if b % interval == 0 and b > 0:\n",
    "            loss_interval = loss_total / interval\n",
    "            time_delta = time.time() - time_start\n",
    "            print(f\"epoch {ep}, {b}/{len(training_data)//max_seq_len} batches, training loss {loss_interval:.2f}, training perplexity {math.exp(loss_interval):.2f}\")\n",
    "            loss_total = 0\n",
    "            time_start = time.time()\n",
    "\n",
    "def eval_model(eval_model_obj, eval_data_source):\n",
    "    eval_model_obj.eval() \n",
    "    loss_total = 0.\n",
    "    num_tokens = len(TEXT.vocab.stoi)\n",
    "    with torch.no_grad():\n",
    "        for j in range(0, eval_data_source.size(0) - 1, max_seq_len):\n",
    "            eval_data, eval_label = return_batch(eval_data_source, j)\n",
    "            op = eval_model_obj(eval_data)\n",
    "            op_flat = op.view(-1, num_tokens)\n",
    "            loss_total += len(eval_data) * loss_func(op_flat, eval_label).item()\n",
    "    return loss_total / (len(eval_data_source) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, 100/1018 batches, training loss 8.50, training perplexity 4901.66\n",
      "epoch 1, 200/1018 batches, training loss 7.16, training perplexity 1286.24\n",
      "epoch 1, 300/1018 batches, training loss 6.76, training perplexity 865.43\n",
      "epoch 1, 400/1018 batches, training loss 6.55, training perplexity 702.21\n",
      "epoch 1, 500/1018 batches, training loss 6.45, training perplexity 631.90\n",
      "epoch 1, 600/1018 batches, training loss 6.31, training perplexity 548.01\n",
      "epoch 1, 700/1018 batches, training loss 6.25, training perplexity 516.28\n",
      "epoch 1, 800/1018 batches, training loss 6.11, training perplexity 450.42\n",
      "epoch 1, 900/1018 batches, training loss 6.09, training perplexity 441.72\n",
      "epoch 1, 1000/1018 batches, training loss 6.08, training perplexity 436.78\n",
      "\n",
      "epoch 1, validation loss 5.82, validation perplexity 336.19\n",
      "\n",
      "epoch 2, 100/1018 batches, training loss 5.98, training perplexity 394.64\n",
      "epoch 2, 200/1018 batches, training loss 5.90, training perplexity 364.08\n",
      "epoch 2, 300/1018 batches, training loss 5.82, training perplexity 337.72\n",
      "epoch 2, 400/1018 batches, training loss 5.78, training perplexity 324.68\n",
      "epoch 2, 500/1018 batches, training loss 5.82, training perplexity 335.50\n",
      "epoch 2, 600/1018 batches, training loss 5.77, training perplexity 319.43\n",
      "epoch 2, 700/1018 batches, training loss 5.78, training perplexity 322.60\n",
      "epoch 2, 800/1018 batches, training loss 5.65, training perplexity 283.28\n",
      "epoch 2, 900/1018 batches, training loss 5.67, training perplexity 291.07\n",
      "epoch 2, 1000/1018 batches, training loss 5.71, training perplexity 300.54\n",
      "\n",
      "epoch 2, validation loss 5.53, validation perplexity 251.09\n",
      "\n",
      "epoch 3, 100/1018 batches, training loss 5.67, training perplexity 288.79\n",
      "epoch 3, 200/1018 batches, training loss 5.59, training perplexity 268.81\n",
      "epoch 3, 300/1018 batches, training loss 5.55, training perplexity 257.23\n",
      "epoch 3, 400/1018 batches, training loss 5.52, training perplexity 249.65\n",
      "epoch 3, 500/1018 batches, training loss 5.55, training perplexity 257.02\n",
      "epoch 3, 600/1018 batches, training loss 5.52, training perplexity 249.50\n",
      "epoch 3, 700/1018 batches, training loss 5.53, training perplexity 252.90\n",
      "epoch 3, 800/1018 batches, training loss 5.39, training perplexity 219.61\n",
      "epoch 3, 900/1018 batches, training loss 5.44, training perplexity 230.41\n",
      "epoch 3, 1000/1018 batches, training loss 5.49, training perplexity 241.15\n",
      "\n",
      "epoch 3, validation loss 5.37, validation perplexity 215.04\n",
      "\n",
      "epoch 4, 100/1018 batches, training loss 5.46, training perplexity 235.42\n",
      "epoch 4, 200/1018 batches, training loss 5.40, training perplexity 220.81\n",
      "epoch 4, 300/1018 batches, training loss 5.36, training perplexity 213.61\n",
      "epoch 4, 400/1018 batches, training loss 5.34, training perplexity 208.66\n",
      "epoch 4, 500/1018 batches, training loss 5.37, training perplexity 213.88\n",
      "epoch 4, 600/1018 batches, training loss 5.35, training perplexity 210.60\n",
      "epoch 4, 700/1018 batches, training loss 5.36, training perplexity 213.75\n",
      "epoch 4, 800/1018 batches, training loss 5.21, training perplexity 183.18\n",
      "epoch 4, 900/1018 batches, training loss 5.26, training perplexity 193.41\n",
      "epoch 4, 1000/1018 batches, training loss 5.32, training perplexity 205.22\n",
      "\n",
      "epoch 4, validation loss 5.31, validation perplexity 202.42\n",
      "\n",
      "epoch 5, 100/1018 batches, training loss 5.31, training perplexity 202.77\n",
      "epoch 5, 200/1018 batches, training loss 5.25, training perplexity 189.64\n",
      "epoch 5, 300/1018 batches, training loss 5.22, training perplexity 184.80\n",
      "epoch 5, 400/1018 batches, training loss 5.20, training perplexity 181.18\n",
      "epoch 5, 500/1018 batches, training loss 5.22, training perplexity 185.54\n",
      "epoch 5, 600/1018 batches, training loss 5.21, training perplexity 182.95\n",
      "epoch 5, 700/1018 batches, training loss 5.22, training perplexity 185.69\n",
      "epoch 5, 800/1018 batches, training loss 5.07, training perplexity 158.79\n",
      "epoch 5, 900/1018 batches, training loss 5.13, training perplexity 169.36\n",
      "epoch 5, 1000/1018 batches, training loss 5.19, training perplexity 179.63\n",
      "\n",
      "epoch 5, validation loss 5.23, validation perplexity 186.53\n",
      "\n"
     ]
    }
   ],
   "source": [
    "min_validation_loss = float(\"inf\")\n",
    "eps = 5\n",
    "best_model_so_far = None\n",
    "\n",
    "for ep in range(1, eps + 1):\n",
    "    ep_time_start = time.time()\n",
    "    train_model()\n",
    "    validation_loss = eval_model(transformer_model, validation_data)\n",
    "    print()\n",
    "    print(f\"epoch {ep:}, validation loss {validation_loss:.2f}, validation perplexity {math.exp(validation_loss):.2f}\")\n",
    "    print()\n",
    "\n",
    "    if validation_loss < min_validation_loss:\n",
    "        min_validation_loss = validation_loss\n",
    "        best_model_so_far = transformer_model\n",
    "\n",
    "    sched_module.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides the cross-entropy loss, the perplexity is also reported. Perplexity is a popularly used metric in natural language processing to indicate how well a probability distribution (a language model, in our case) fits or predicts a sample. The lower the perplexity, the better the model is at predicting the sample. Mathematically, perplexity is just the exponential of the cross-entropy loss. Intuitively, this metric is used to indicate how perplexed or confused the model is while making predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing loss 5.14, testing perplexity 171.47\n"
     ]
    }
   ],
   "source": [
    "testing_loss = eval_model(best_model_so_far, testing_data)\n",
    "print(f\"testing loss {testing_loss:.2f}, testing perplexity {math.exp(testing_loss):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we built a transformer model using PyTorch for the task of language modeling. We explored the transformer architecture in detail and how it is implemented in PyTorch. We used the WikiText-2 dataset and torchtext functionalities to load and process the dataset. We then trained the transformer model for 5 epochs and evaluated it on a separate test set. This shall provide us with all the information we need to get started on working with transformers.\n",
    "\n",
    "Besides the original transformer model, which was devised in 2017, a number of successors have since been developed over the years, especially around the field of language modeling, such as the following:\n",
    "\n",
    "- Bidirectional Encoder Representations from Transformers (BERT), 2018\n",
    "- Generative Pretrained Transformer (GPT), 2018\n",
    "- GPT-2, 2019\n",
    "- Conditional Transformer Language Model (CTRL), 2019\n",
    "- Transformer-XL, 2019\n",
    "- Distilled BERT (DistilBERT), 2019\n",
    "- Robustly optimized BERT pretraining Approach (RoBERTa), 2019\n",
    "- GPT-3, 2020\n",
    "\n",
    "While we will not cover these models in detail in this chapter, you can nonetheless get started with using these models with PyTorch thanks to the transformers library, developed by HuggingFace (https://github.com/huggingface/transformers). It provides pre-trained transformer family models for various tasks, such as language modeling, text classification, translation, question-answering, and so on.\n",
    "\n",
    "Besides the models themselves, it also provides tokenizers for the respective models. For example, if we wanted to use a pre-trained BERT model for language modeling, we would need to write the following code once we have installed the transformers library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertForMaskedLM, BertTokenizer\n",
    "bert_model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "token_gen = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "ip_sequence = token_gen(\"I love PyTorch !\", return_tensors=\"pt\")[\"input_ids\"]\n",
    "op = bert_model(ip_sequence, labels=ip_sequence)\n",
    "total_loss, raw_preds = op[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, it takes just a couple of lines to get started with a BERT-based language model. This demonstrates the power of the PyTorch ecosystem. You are encouraged to explore this with more complex variants, such as Distilled BERT or RoBERTa, using the transformers library. For more details, please refer to their GitHub page, which was mentioned previously.\n",
    "\n",
    "This concludes our exploration of transformers. We did this by both building one from scratch as well as by reusing pre-trained models. The invention of transformers in the natural language processing space has been paralleled with the ImageNet moment in the field of computer vision, so this is going to be an active area of research. PyTorch will have a crucial role to play in the research and deployment of these types of models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
